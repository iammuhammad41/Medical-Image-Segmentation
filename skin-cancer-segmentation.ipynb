{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iammuhammad41/Medical-Image-Segmentation/blob/main/skin-cancer-segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "55fffdda",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2025-06-26T06:53:50.258293Z",
          "iopub.status.busy": "2025-06-26T06:53:50.257381Z",
          "iopub.status.idle": "2025-06-26T06:54:04.440911Z",
          "shell.execute_reply": "2025-06-26T06:54:04.439657Z"
        },
        "papermill": {
          "duration": 14.189952,
          "end_time": "2025-06-26T06:54:04.442325",
          "exception": false,
          "start_time": "2025-06-26T06:53:50.252373",
          "status": "completed"
        },
        "tags": [],
        "id": "55fffdda"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "\n",
        "DATASET_PATH = \"/kaggle/input/a0-2025-medical-image-segmentation/Dataset\"\n",
        "\n",
        "TRAIN_IMAGE_DIR = os.path.join(DATASET_PATH, \"Train/Image\")\n",
        "TRAIN_MASK_DIR = os.path.join(DATASET_PATH, \"Train/Mask\")\n",
        "TEST_DIR = os.path.join(DATASET_PATH, \"Test/Image\")\n",
        "\n",
        "\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "\n",
        "# !ls -R \"{DATASET_PATH}\"\n",
        "\n",
        "\n",
        "print(\"\\nGPU check:\")\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if 'failed' in gpu_info or 'command not found' in gpu_info:\n",
        "  print('-> There is no gpu...')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d0919559",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-26T06:54:04.449316Z",
          "iopub.status.busy": "2025-06-26T06:54:04.448915Z",
          "iopub.status.idle": "2025-06-26T06:54:05.211180Z",
          "shell.execute_reply": "2025-06-26T06:54:05.210229Z"
        },
        "papermill": {
          "duration": 0.77078,
          "end_time": "2025-06-26T06:54:05.216229",
          "exception": false,
          "start_time": "2025-06-26T06:54:04.445449",
          "status": "completed"
        },
        "tags": [],
        "id": "d0919559",
        "outputId": "3757ccad-b6cb-400a-d560-7ebb44954550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total image-mask pairs found: 1087\n",
            "Number of training pairs: 869\n",
            "Number of validation pairs: 218\n"
          ]
        }
      ],
      "source": [
        "mask_files = sorted([f for f in os.listdir(TRAIN_MASK_DIR) if f.lower().endswith('.png')])\n",
        "\n",
        "\n",
        "image_mask_pairs = [\n",
        "    (os.path.join(TRAIN_MASK_DIR, name), os.path.join(TRAIN_IMAGE_DIR, name.replace(\".png\", \".jpg\")))\n",
        "    for name in mask_files\n",
        "]\n",
        "\n",
        "print(f\"Total image-mask pairs found: {len(image_mask_pairs)}\")\n",
        "\n",
        "\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(image_mask_pairs)\n",
        "\n",
        "split_idx = int(len(image_mask_pairs) * 0.8)\n",
        "train_pairs = image_mask_pairs[:split_idx]\n",
        "val_pairs = image_mask_pairs[split_idx:]\n",
        "\n",
        "\n",
        "print(f\"Number of training pairs: {len(train_pairs)}\")\n",
        "print(f\"Number of validation pairs: {len(val_pairs)}\")\n",
        "print(f\"\\nExample training pair (Mask, Image): {train_pairs[0]}\")\n",
        "\n",
        "\n",
        "print(\"\\nDisplaying a sample pair from the training set...\")\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "\n",
        "mask_sample = Image.open(train_pairs[0][0])\n",
        "axs[0].imshow(mask_sample, cmap=\"gray\")\n",
        "axs[0].set_title(\"Image Mask\")\n",
        "axs[0].axis(\"off\")\n",
        "\n",
        "\n",
        "image_sample = Image.open(train_pairs[0][1])\n",
        "axs[1].imshow(image_sample)\n",
        "axs[1].set_title(\"Original Image\")\n",
        "axs[1].axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a52c8b3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-26T06:54:05.231133Z",
          "iopub.status.busy": "2025-06-26T06:54:05.230836Z",
          "iopub.status.idle": "2025-06-26T06:54:06.594995Z",
          "shell.execute_reply": "2025-06-26T06:54:06.593728Z"
        },
        "papermill": {
          "duration": 1.373237,
          "end_time": "2025-06-26T06:54:06.596466",
          "exception": false,
          "start_time": "2025-06-26T06:54:05.223229",
          "status": "completed"
        },
        "tags": [],
        "id": "8a52c8b3",
        "outputId": "16cba76c-48b4-4c60-ec2a-2906bc7861f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataLoaders created successfully.\n",
            "Image batch shape from train_loader: torch.Size([8, 3, 256, 256])\n",
            "Mask batch shape from train_loader: torch.Size([8, 1, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "\n",
        "train_transforms = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.Rotate(limit=35, p=0.5),\n",
        "    A.ColorJitter(p=0.2),\n",
        "    A.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "val_transforms = A.Compose([\n",
        "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "    A.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "        max_pixel_value=255.0,\n",
        "    ),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for loading image-mask pairs.\n",
        "    It uses Albumentations for synchronized transformations on both\n",
        "    the image and the mask.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_mask_pairs, transform=None):\n",
        "        self.image_mask_pairs = image_mask_pairs\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_mask_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mask_path, image_path = self.image_mask_pairs[idx]\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "\n",
        "        mask = (mask / 255.0 > 0.5).astype(np.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, mask=mask)\n",
        "            image = transformed['image']\n",
        "            mask = transformed['mask']\n",
        "            mask = mask.unsqueeze(0)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = SegmentationDataset(\n",
        "    image_mask_pairs=train_pairs,\n",
        "    transform=train_transforms\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "\n",
        "val_dataset = SegmentationDataset(\n",
        "    image_mask_pairs=val_pairs,\n",
        "    transform=val_transforms\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    print(\"DataLoaders created successfully.\")\n",
        "    images, masks = next(iter(train_loader))\n",
        "    print(f\"Image batch shape from train_loader: {images.shape}\")\n",
        "    print(f\"Mask batch shape from train_loader: {masks.shape}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while verifying the DataLoader: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7147ceb5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-26T06:54:06.611319Z",
          "iopub.status.busy": "2025-06-26T06:54:06.610593Z",
          "iopub.status.idle": "2025-06-26T06:54:08.037922Z",
          "shell.execute_reply": "2025-06-26T06:54:08.036913Z"
        },
        "papermill": {
          "duration": 1.436162,
          "end_time": "2025-06-26T06:54:08.039410",
          "exception": false,
          "start_time": "2025-06-26T06:54:06.603248",
          "status": "completed"
        },
        "tags": [],
        "id": "7147ceb5"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"A block consisting of two sequential convolutional layers,\n",
        "    each followed by BatchNorm and ReLU activation.\n",
        "    (Convolution => [BatchNorm] => ReLU) * 2\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "\n",
        "class RollasUnet(nn.Module):\n",
        "    \"\"\"\n",
        "    The U-Net architecture for image segmentation.\n",
        "    It consists of an encoder (contracting path), a bottleneck,\n",
        "    and a decoder (expansive path) with skip connections.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(RollasUnet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "\n",
        "        self.inc = DoubleConv(in_channels, 64)\n",
        "        self.down1 = DoubleConv(64, 128)\n",
        "        self.down2 = DoubleConv(128, 256)\n",
        "        self.down3 = DoubleConv(256, 512)\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "\n",
        "        self.bottleneck = DoubleConv(512, 1024)\n",
        "\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.conv1 = DoubleConv(1024, 512)\n",
        "\n",
        "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.conv2 = DoubleConv(512, 256)\n",
        "\n",
        "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.conv3 = DoubleConv(256, 128)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.conv4 = DoubleConv(128, 64)\n",
        "\n",
        "\n",
        "        self.outc = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        skip1 = self.inc(x)\n",
        "        skip2 = self.down1(self.pool(skip1))\n",
        "        skip3 = self.down2(self.pool(skip2))\n",
        "        skip4 = self.down3(self.pool(skip3))\n",
        "\n",
        "\n",
        "        x = self.bottleneck(self.pool(skip4))\n",
        "\n",
        "\n",
        "        x = self.up1(x)\n",
        "        x = torch.cat([x, skip4], dim=1)\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.up2(x)\n",
        "        x = torch.cat([x, skip3], dim=1)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        x = self.up3(x)\n",
        "        x = torch.cat([x, skip2], dim=1)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        x = self.up4(x)\n",
        "        x = torch.cat([x, skip1], dim=1)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "\n",
        "        logits = self.outc(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "model = RollasUnet(in_channels=3, out_channels=1).to(device)\n",
        "\n",
        "\n",
        "dummy_input = torch.randn(BATCH_SIZE, 3, IMAGE_HEIGHT, IMAGE_WIDTH).to(device)\n",
        "\n",
        "\n",
        "preds = model(dummy_input)\n",
        "\n",
        "\n",
        "print(\"--- Model Sanity Check ---\")\n",
        "print(f\"Input tensor shape:  {dummy_input.shape}\")\n",
        "print(f\"Output tensor shape: {preds.shape}\")\n",
        "print(f\"Model successfully created and tested.\")\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ac7bb34d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-26T06:54:08.053663Z",
          "iopub.status.busy": "2025-06-26T06:54:08.053388Z",
          "iopub.status.idle": "2025-06-26T07:19:18.567449Z",
          "shell.execute_reply": "2025-06-26T07:19:18.566216Z"
        },
        "papermill": {
          "duration": 1510.52279,
          "end_time": "2025-06-26T07:19:18.568943",
          "exception": false,
          "start_time": "2025-06-26T06:54:08.046153",
          "status": "completed"
        },
        "tags": [],
        "id": "ac7bb34d"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements Dice Loss, calculated as 1 - Dice Coefficient.\n",
        "    Useful for directly optimizing the Dice score metric.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(DiceLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        inputs = torch.sigmoid(inputs)\n",
        "\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        intersection = (inputs * targets).sum()\n",
        "        dice_coeff = (2. * intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
        "\n",
        "        return 1 - dice_coeff\n",
        "\n",
        "def dice_coefficient(preds, targets, smooth=1e-6):\n",
        "    \"\"\"\n",
        "    Calculates the Dice Coefficient metric for evaluation.\n",
        "    Args:\n",
        "        preds (torch.Tensor): The model's raw output logits.\n",
        "        targets (torch.Tensor): The ground truth masks.\n",
        "    Returns:\n",
        "        float: The Dice Coefficient score.\n",
        "    \"\"\"\n",
        "    preds = torch.sigmoid(preds)\n",
        "    preds = (preds > 0.5).float()\n",
        "    intersection = (preds * targets).sum()\n",
        "    dice = (2. * intersection + smooth) / (preds.sum() + targets.sum() + smooth)\n",
        "    return dice.item()\n",
        "\n",
        "def combined_loss(pred, target):\n",
        "    \"\"\"\n",
        "    A combined loss function that balances between BCE and Dice loss.\n",
        "    This helps with both pixel-level accuracy and segmentation overlap.\n",
        "    \"\"\"\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    dice = DiceLoss()\n",
        "    return 0.5 * bce(pred, target) + 0.5 * dice(pred, target)\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 25\n",
        "\n",
        "model = RollasUnet(in_channels=3, out_channels=1).to(DEVICE)\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_dice_scores = []\n",
        "best_val_dice = 0.0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"--- Starting Training ---\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Training]\", leave=False)\n",
        "\n",
        "    for data, targets in train_loop:\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.float().to(device=DEVICE)\n",
        "\n",
        "        predictions = model(data)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    total_val_dice = 0.0\n",
        "    total_val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data = data.to(device=DEVICE)\n",
        "            targets = targets.float().to(device=DEVICE)\n",
        "            predictions = model(data)\n",
        "\n",
        "            val_loss = loss_fn(predictions, targets)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "            dice_score = dice_coefficient(predictions, targets)\n",
        "            total_val_dice += dice_score\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    avg_val_dice = total_val_dice / len(val_loader)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_dice_scores.append(avg_val_dice)\n",
        "\n",
        "    if avg_val_dice > best_val_dice:\n",
        "        best_val_dice = avg_val_dice\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(f\"--> New best model saved with Dice score: {avg_val_dice:.4f}\")\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | \"\n",
        "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
        "          f\"Val Loss: {avg_val_loss:.4f} | \"\n",
        "          f\"Val Dice: {avg_val_dice:.4f}\")\n",
        "\n",
        "print(\"\\n--- Training Finished ---\")\n",
        "print(f\"Best validation Dice score achieved: {best_val_dice:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "99c1df3b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-26T07:19:19.072572Z",
          "iopub.status.busy": "2025-06-26T07:19:19.072220Z",
          "iopub.status.idle": "2025-06-26T07:19:21.560910Z",
          "shell.execute_reply": "2025-06-26T07:19:21.560046Z"
        },
        "papermill": {
          "duration": 2.759343,
          "end_time": "2025-06-26T07:19:21.577813",
          "exception": false,
          "start_time": "2025-06-26T07:19:18.818470",
          "status": "completed"
        },
        "tags": [],
        "id": "99c1df3b"
      },
      "outputs": [],
      "source": [
        "def visualize_predictions(model, loader, device, num_images_to_show=3):\n",
        "    \"\"\"\n",
        "    Visualizes model predictions against the ground truth masks.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The trained model.\n",
        "        loader (DataLoader): The DataLoader to get samples from (e.g., val_loader).\n",
        "        device (str): The device to run the model on (\"cuda\" or \"cpu\").\n",
        "        num_images_to_show (int): The number of sample images to display.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Visualizing a few predictions... ---\")\n",
        "    model.eval()\n",
        "\n",
        "    try:\n",
        "        images, masks = next(iter(loader))\n",
        "    except StopIteration:\n",
        "        print(\"DataLoader is empty. Cannot visualize.\")\n",
        "        return\n",
        "\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        preds = torch.sigmoid(model(images))\n",
        "\n",
        "        preds = (preds > 0.5).float()\n",
        "\n",
        "\n",
        "    images = images.cpu().numpy()\n",
        "    masks = masks.cpu().numpy()\n",
        "    preds = preds.cpu().numpy()\n",
        "\n",
        "    num_to_show = min(num_images_to_show, len(images))\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(num_to_show, 3, figsize=(15, num_to_show * 5))\n",
        "\n",
        "    for i in range(num_to_show):\n",
        "\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "        img_display = np.transpose(images[i], (1, 2, 0))\n",
        "\n",
        "        img_display = std * img_display + mean\n",
        "\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "\n",
        "        ax_row = axs[i] if num_to_show > 1 else axs\n",
        "\n",
        "        ax_row[0].imshow(img_display)\n",
        "        ax_row[0].set_title(\"Input Image\")\n",
        "        ax_row[0].axis(\"off\")\n",
        "\n",
        "        ax_row[1].imshow(masks[i].squeeze(), cmap=\"gray\")\n",
        "        ax_row[1].set_title(\"Ground Truth Mask\")\n",
        "        ax_row[1].axis(\"off\")\n",
        "\n",
        "        ax_row[2].imshow(preds[i].squeeze(), cmap=\"gray\")\n",
        "        ax_row[2].set_title(\"Predicted Mask\")\n",
        "        ax_row[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "visualize_predictions(model, val_loader, device=DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a498d632",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-26T07:19:22.121827Z",
          "iopub.status.busy": "2025-06-26T07:19:22.121321Z",
          "iopub.status.idle": "2025-06-26T07:19:22.134051Z",
          "shell.execute_reply": "2025-06-26T07:19:22.133225Z"
        },
        "papermill": {
          "duration": 0.289142,
          "end_time": "2025-06-26T07:19:22.135308",
          "exception": false,
          "start_time": "2025-06-26T07:19:21.846166",
          "status": "completed"
        },
        "tags": [],
        "id": "a498d632"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for loading test images.\n",
        "    It returns the transformed image, its filename, and its original dimensions.\n",
        "    \"\"\"\n",
        "    def __init__(self, test_dir, transform=None):\n",
        "        self.test_dir = test_dir\n",
        "        self.image_names = sorted(os.listdir(test_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_name = self.image_names[idx]\n",
        "        image_path = os.path.join(self.test_dir, image_name)\n",
        "\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        original_height, original_width, _ = image.shape\n",
        "\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image)\n",
        "            image = transformed['image']\n",
        "\n",
        "        return image, image_name, (original_height, original_width)\n",
        "\n",
        "\n",
        "\n",
        "def generate_submission_and_visualize(model, device, test_dir, num_to_show=5):\n",
        "    \"\"\"\n",
        "    Uses the trained model to generate predictions on the test set,\n",
        "    creates a submission file, and visualizes some results.\n",
        "    \"\"\"\n",
        "    test_transforms = A.Compose([\n",
        "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
        "        ToTensorV2(),\n",
        "    ])\n",
        "\n",
        "    test_dataset = TestDataset(test_dir, transform=test_transforms)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "\n",
        "\n",
        "    images_to_display = []\n",
        "    masks_to_display = []\n",
        "\n",
        "    print(\"\\n--- Generating predictions on the test set... ---\")\n",
        "    with torch.no_grad():\n",
        "        for images, image_names, original_dims in tqdm(test_loader, desc=\"Predicting\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            preds = torch.sigmoid(model(images))\n",
        "            preds = (preds > 0.5).cpu().numpy()\n",
        "\n",
        "            for i in range(len(preds)):\n",
        "                pred_mask_256 = preds[i].squeeze().astype(np.uint8)\n",
        "                original_h, original_w = original_dims[0][i].item(), original_dims[1][i].item()\n",
        "                resized_mask = cv2.resize(pred_mask_256, (original_w, original_h), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "\n",
        "\n",
        "                if len(images_to_display) < num_to_show:\n",
        "                    original_image_path = os.path.join(test_dir, image_names[i])\n",
        "                    images_to_display.append(original_image_path)\n",
        "                    masks_to_display.append(resized_mask)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\n--- Visualizing a few test predictions (in original size) ---\")\n",
        "    for i in range(len(images_to_display)):\n",
        "        original_image = cv2.imread(images_to_display[i])\n",
        "        original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
        "        predicted_mask = masks_to_display[i]\n",
        "\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(15, 7))\n",
        "        axs[0].imshow(original_image)\n",
        "        axs[0].set_title(f\"Original Test Image: {os.path.basename(images_to_display[i])}\")\n",
        "        axs[0].axis('off')\n",
        "\n",
        "        axs[1].imshow(predicted_mask, cmap='gray')\n",
        "        axs[1].set_title(\"Predicted Mask (Resized)\")\n",
        "        axs[1].axis('off')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ff1f8194",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-06-26T07:19:22.712458Z",
          "iopub.status.busy": "2025-06-26T07:19:22.711527Z",
          "iopub.status.idle": "2025-06-26T07:19:30.113733Z",
          "shell.execute_reply": "2025-06-26T07:19:30.112705Z"
        },
        "papermill": {
          "duration": 7.666391,
          "end_time": "2025-06-26T07:19:30.119769",
          "exception": false,
          "start_time": "2025-06-26T07:19:22.453378",
          "status": "completed"
        },
        "tags": [],
        "id": "ff1f8194"
      },
      "outputs": [],
      "source": [
        "model = RollasUnet(in_channels=3, out_channels=1).to(DEVICE)\n",
        "\n",
        "print(\"Loading best model weights from 'best_model.pth'...\")\n",
        "try:\n",
        "    model.load_state_dict(torch.load(\"best_model.pth\", map_location=DEVICE))\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'best_model.pth' not found. Please ensure the model was trained and saved correctly.\")\n",
        "else:\n",
        "    generate_submission_and_visualize(model, DEVICE, TEST_DIR)"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 11654191,
          "sourceId": 97838,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 1548.500385,
      "end_time": "2025-06-26T07:19:34.022547",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-06-26T06:53:45.522162",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}